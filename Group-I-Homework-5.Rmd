---
title: "Homework 5"
author: "Group I: Demirbilek, Taroni, Zagatti"
date: "Spring 2020"
output:
  html_document:
    toc: no
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
fontsize: 10pt
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/', fig.height = 10, fig.width = 10)
```

```{r setup, include=FALSE}
library(MASS)
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

# {.tabset}

## DAAG {.tabset}


### Exercise 4.21


### Exercise 7.2

**Use anova() to compare the two models: 
`roller.lm <- lm(depression ~ weight, data = roller)`, `roller.lm2 <- lm(depression ~ weight + I(weigth^2), data = roller)`
Is there any justification for including the squared term?**


*solution :* 

```{r, echo=TRUE,warning=FALSE}
library(DAAG)
#help(roller)
roller.lm <- lm(depression ~ weight, data = roller)
roller.lm2 <- lm(depression ~ weight + I(weight^2), data = roller)
anova(roller.lm , roller.lm2)

```
These two models differ only in the use of `weight^2`. So ANOVA will test whether or not including this leads significant improvement. Adding this term spawned non-significant result (p=0.259). Thus we should reject second model, in other words adding `weight^2`didn't lead better result.

## CS {.tabset}


### Exercise 3.2
**Rewrite the following to eliminate the loops, first using `apply` and then
using `rowSums`:**

```{r, echo=TRUE,warning=FALSE}
X <- matrix(runif(100000),1000,100); 
z <- rep(0,1000) 
for (i in 1:1000) {
for (j in 1:100) {z[i] <- z[i] + X[i,j]} }

```

Confirm that all three versions give the same answers, but that your rewrites
are much faster than the original. (`system.time` is a useful function.)


*solution :* 
In order to test how fast each version is we use the function `system.time` that determines how much real and CPU time (in seconds) the currently running R process takes to execute.

In order to check that all three processes provided the same results we use the function `identical` that returns a logical value `TRUE` if the two arguments are exactly the same

```{r, echo=TRUE,warning=FALSE}
# Definition of the used variables
X <- matrix(runif(100000),1000,100)
z <- rep(0,1000)

# Time Evaluation for the first version
system.time( for (i in 1:1000) {
for (j in 1:100) {
z[i] <- z[i]+X[i,j]  }})

# Time evaluation of the second version (apply function)
system.time(z1 <- apply(X, 1, sum))

# Check between the first and second result
identical(z,z1)

# Time evaluation of the third version (rowSums function)
system.time(z2 <- rowSums(X))

# Check between the first and third result
identical(z,z2)

```

From the results of the checks obtained with `identical` we see that both `apply` and `rowSums` provide the same result al the first version proprosed. Comparing the results of `system.time` it is clear that the two solutions proposed are much faster then the first one, in particular `rowSums` provides to be the most efficient.

### Exercise 4.4

**Suppose that you have $n$ independent measurements of times between major aircraft disasters, $t_i$ , and believe that the probability density function for the $t_i$’s is of the form: $f(t) = ke^{−\lambda t^2}$ $t \geq 0$ where $λ$ and $k$ are the same for all $i$.**

**(a) By considering normal p.d.f., show that  $k = \sqrt{4 \lambda/\pi}.$**

*solution (a):*

$$f(t) = \frac{1}{\sigma\sqrt{2\pi}}\exp\{-\frac{(x - \mu)^2}{2\sigma^2}\} = ke^{-\lambda t^2}$$
$k = \frac{1}{\sigma\sqrt{2\pi}}$ , $\lambda = \frac{1}{2\sigma^2}$ and $t = (x - \mu)$. Here we can find that

$$
k = \frac{1}{\sqrt{2\pi \sigma^2}} = \sqrt{\frac{\lambda}{\pi}}
$$

**(b) Obtain a maximum likelihood estimator for $\lambda$.**

*solution (b):*

Likelihood of the given expression is the following

$$
L(t) = \prod_{i=1}^{n}ke^{−\lambda t_i^2}
$$
The log likelihood function carries the same information of the likelihood function, but it is mor manageable so for simplicity we are going to use log likelihood.

$$
\ell(t) = n \log(k) -\lambda \sum_{i=1}^{n}t_i^2 = n \log(\sqrt{\lambda/ \pi} ) -\lambda \sum_{i=1}^{n}t_i^2 
$$
To find maximum likelihood estimator of $\lambda$ we need to derivate the log likelihood wit respect to $\lambda$ and find the $\lambda$ by setting equation zero.

$$
\frac{d\ell}{d\lambda} = \frac{n}{2\lambda}-\sum_{i=1}^{n}t_i^2 \\
\lambda = \frac{n}{2\sum_{i=1}^{n}t_i^2}
$$


**(c) Given observation of $T_i$ (in days) of 243, 14, 121, 63, 45, 407 and 34  use a generalised likelihood ratio test to
test $H_0 : \lambda = 10^{-4}$ against the alternative of no restriction on $\lambda$ at the $5%$ significance level. Note that if $V \sim X_1^2$ then $Pr[V \leq 3.841] = 0.95$.**

*solution (c):*

Likelihood test statistic is given by $W(\lambda_0) = 2(\ell(\hat{\lambda})-\ell(\lambda_0))$

```{r echo=TRUE,warning=FALSE}
log_lik_aircraft <- function(data,lambda){
  length(data) * log(sqrt(lambda/pi)) - lambda * sum(data^2)
}

data <-  c(243, 14, 121, 63, 45, 407 ,34)
mle <- length(data) / (2 * sum(data^2))
cat("Maximum likelihood estimation for lambda:",mle)

lambda_0 <- log_lik_aircraft(data,10^-4)
lambda_0
# since there is no restriction on lambda mle is chosen
lambda_mle <- log_lik_aircraft(data,mle)

lrt <- 2 * (lambda_mle - lambda_0)
# to find p-value by using Pr(W >= w_obs)
cat("p-value for test is:",pchisq(lrt,df=1,lower.tail = FALSE))

```
We have very small p-value, which indicates there is strong evidence against the null hypothesis.

## Bayesian {.tabset}

### BC 2.5

**Suppose you are interested in estimating the average total snowfall per year $\mu$ (in inches) for a large city on the East Coast of the United States. Assume individual yearly snow totals $y_1,\dots, y_n$ are collected from a population that is assumed to be normally distributed with mean $\mu$ and known standard deviation $\sigma = 10$ inches.**

**(a) Before collecting data, suppose you believe that the mean snowfall $\mu$ can be the values 20, 30, 40, 50, 60, and 70 inches with the following probabilities: 0.1, 0.15, 0.25, 0.25, 0.15, 0.1 respectively. Place the values of $\mu$ in the vector `mu` and the associated prior probabilities in the vector `prior`.**

**(b) Suppose you observe the yearly snowfall totals 38.6, 42.4, 57.5, 40.5, 51.7, 67.1, 33.4, 60.9, 64.1, 40.1, 40.7, and 6.4 inches. Enter these data into a vector `y` and compute the sample mean `ybar`.**

**(c) In this problem, the likelihood function is given by** 

$$L(\mu) \propto \exp \biggl(-\frac{n}{2\sigma^2}(\mu-\bar{y})^2\biggr)$$
**where $\bar{y}$ is the sample mean. Compute the likelihood on the list of values in `mu` and place the likelihood values in the vector `like`.** 

**(d) One can compute the posterior probabilities for $\mu$ using the formula**

$$ post=prior*like/sum(prior*like)$$
**Compute the posterior probabilities of $\mu$ for this example.** 

**(e) Using the function `discint`, find an 80\% probability interval for $\mu$**

*solution:*

```{r echo=TRUE,warning=FALSE}
library('LearnBayes')
# main data
mu <- c(20,30,40,50,60,70)
mu <- array(mu)
prior <- c(0.1,0.15,0.25,0.25,0.15,0.1)
prior <- array(prior)
y <- c(38.6, 42.4, 57.5, 40.5, 51.7, 67.1, 33.4, 60.9, 64.1, 40.1, 6.4)
y <- array(y)
n <- dim(y)
sigma <- 10
ybar <- mean(y)
# likelihood
like <- exp(- n/(2*sigma^2)*(mu-ybar)^2)
# posterior
post <- prior*like/sum(prior*like)
# Build the discrete probability distribution
probdist <- cbind(mu,post)
print(probdist)
# Compute the 80% probability interval for mu
discint(probdist, 0.8)

```

`set` is the set of values of the probability interval and `prob` is the probability content of the interval.